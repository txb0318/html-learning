### 机器学习

9. 支持向量机的对偶问题

        [支持向量机的对偶问题]([(7条消息) 支持向量机（SVM）中的对偶问题_予亭的博客-CSDN博客_支持向量机对偶问题](https://blog.csdn.net/randompeople/article/details/92083294))

        [凸集、凸函数、凸优化问题，线性规划，二次规划]([凸集，凸函数，凸优化问题，线性规划，二次规划，二次约束二次规划，半正定规划 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/263126963))

        **凸优化**：

> 1. 在最小化（最大化）的要求下
> 
> 2. 目标函数是一个凸函数（凹函数）
> 
> 3. 约束条件所形成的可行域集合是一个凸集

        仿射函数

        $F (x) = A x + b$

10. 支持向量机中支撑向量
    
    [支撑向量]([SVM系列第五讲--支撑向量 - 简书 (jianshu.com)](https://www.jianshu.com/p/a2c9ab800946))

<img title="" src="file:///C:/Users/田晓滨/AppData/Roaming/marktext/images/2022-11-17-21-08-35-image.png" alt="" width="292" data-align="center">

11. 过拟合和欠拟合
    
    > 过拟合：当学习器把训练样本**学的“太好”了的**时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降，这种现象称为过拟合。
    > 
    > 欠拟合：欠拟合是指对训练样本的一般性质尚未学好。在训练集及测试集上的表现都不好

        [过拟合和欠拟合]([(7条消息) 过拟合与欠拟合_Minouio的博客-CSDN博客](https://blog.csdn.net/qq_42012732/article/details/107318550))

12. 决策树节点划分
    
    > <font color=Red>ID3使用最大信息熵增益</font>
    > 
    > 信息熵Entropy或Info。熵反应了物体内部的混乱程度，而信息熵反应了不同类的样本的占比，也就是节点的纯度。信息熵越大，说明样本的分布节点的纯度越低，信息量越少；相反，信息熵越小，说明样本的角度越高，信息量越大。
    > 
    > **根据信息熵的含义，我们知道我们要选择能使节点内部纯度高、也就是信息熵最小的特征作为划分依据，我们就计算以某个特征划分自己之后当前节点划分之后各个节点的信息熵之和，并且要是这个和与当前节点划分前的信息熵减小的尽可能多。而这个差也就是信息增益**
    > 
    > $\color{red}{C4.5使用信息增益比率}$
    > 
    > 增益比率在信息熵增益的基础上除以每个划分的信息熵(Split Information)，从而消除这种有大量的子节点、但每个子节点的样本数量很少的情况的影响。
    > 
    > $\color{green}{CART使用基尼指数}$
    > 
    > CART使用基尼系数（GINI）作为划分依据。选择GINI系数最小的特征作为当前的分割数据的特征。基尼系数反映了从样本集中随机抽取两个样本，其类别不一样的概率。基尼系数越大，也就意味着从样本集中随机抽两个样本的类别不相同的概率越大，也就是样本的纯度越小，即对应这信息熵越大。

        [信息熵、不纯度]([(7条消息) 决策树算法笔记整理1 - 如何划分？（信息熵，不纯度及信息增益）_sevieryang的博客-CSDN博客](https://blog.csdn.net/qq_42442369/article/details/86625591))

13. 决策树策略
    
    > 类别非纯度、**梯度下降**、**信息增益率**、**基尼指数**

14. 集成学习中集分类器如何选择使其效率更好
    
    > 在集成学习中，算法不要求每个学习器性能最好，但是期望它们对问题具有不同的看法，即**分类器多样且差异大**

        [分类器的选择]([经典机器学习系列之【集成学习】 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/105038453))

15. 集成学习中每个集分类器的正确率最低要求为50%
    
    > 每个基分类器的错误率都应当低于0.5，否则集成的结果反而会提高错误率。
    > 
    > 每个基分类器应该尽可能相互独立，这是因为如果每个基分类器分类结果差不多，则集成后的分类器整体和单个分类器做出的决策实际上没有什么差异。

        [集成学习前提]([(7条消息) 采用集成学习算法提高分类器的准确性_永永夜的博客-CSDN博客](https://blog.csdn.net/jerr__y/article/details/52955148))

16. Bagging学习特点
    
    > 构造训练集采用Bootstrap方式
    > 
    > 随机有放回采样，每轮训练样本权重相同
    > 
    > 分类器可以并行训练
    > 
    > 预测权重相等

17. Boosting学习特点
    
    > 构造训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整
    > 
    > 根据错误率不断调整样例的权值，错误率越大则权重越大
    > 
    > 每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重
    > 
    > 各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

        [Bagging和Boosting]([从0开始机器学习-Bagging和Boosting - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37730184))

18. 随机森林方法
    
    > 随机森林算法（Random Forest简称RF）是Bagging算法的进阶。其优化：
    > 
    > Bagging的基学习器不一定是同质的，也不一定是决策树；但RF以CART为基学习器。
    > 
    > RF在训练过程中，引入了随机特征选择。RF在Bagging的数据样本扰动的基础上，增加了输入特征扰动，提高了模型的泛化能力。具体来说，传统决策树在选择划分特征时，在当前结点的特征集合中选择一个最优划分特征；而在RF中，是对基决策树的每个结点，先从该结点的特征集合中随机选择一个含有个特征的子集，然后再从该子集选择最优划分特征。越小，模型越健壮，同时对训练集的拟合程度会变差，也就是说，越小，模型方差越小，偏差越大。

19. 软间隔SVM的阈值趋于无穷
    
    >  软间隔：放宽SVM的限制，可以允许存在一些误分类的点
    > 
    > 即C无穷大，在如此高的误分类惩罚下，不会存在软间隔分类超平面，因为一点错误都不可能发生。
    > 
    > 代价参数的大小决定了SVM能允许的误分类程度。
    > C的值小：优化的目标是得到一个尽可能光滑的决策平面。
    > C的值大：模型只允许出现很小数量的误分类点。
    > 它可以简单的看做是对误分类的惩罚。

        [SVM测试]([独家 | 25道SVM题目，测一测你的基础如何?（附资源） - 腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1169308#:~:text=1%20%E5%8F%AA%E8%A6%81%E6%9C%80%E4%BD%B3%E5%88%86%E7%B1%BB%E8%B6%85%E5%B9%B3%E9%9D%A2%E5%AD%98%E5%9C%A8%EF%BC%8C%E5%AE%83%E5%B0%B1%E8%83%BD%E5%B0%86%E6%89%80%E6%9C%89%E6%95%B0%E6%8D%AE%E5%85%A8%E9%83%A8%E6%AD%A3%E7%A1%AE%E5%88%86%E7%B1%BB%202,%E8%BD%AF%E9%97%B4%E9%9A%94SVM%E5%88%86%E7%B1%BB%E5%99%A8%E5%B0%86%E6%AD%A3%E7%A1%AE%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%203%20%E4%BA%8C%E8%80%85%E9%83%BD%E4%B8%8D%E5%AF%B9))

20. 回归问题和分类问题的区别
    
    > 连续即回归，离散即分类

<img title="" src="file:///C:/Users/田晓滨/AppData/Roaming/marktext/images/2022-11-17-23-10-14-image.png" alt="" width="556" data-align="center">

        [分类和回归]([(30 封私信 / 80 条消息) 分类与回归区别是什么？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/21329754))

21. 最小二乘回归方法的等效回归方法
    
    > 最小二乘法是在寻找观测数据与回归超平面之间的误差距离最小的参数。最大似然估计是最大化观测数据发生的概率

        [最小二乘回归和最大似然回归]([最大似然估计：从概率角度理解线性回归的优化目标 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/143416436))

22. 正则化的回归分析

> 当数据量少，特征也少的时候，我们训练的模型是**欠拟合**，这时候我们会通过**交叉验证**来弥补。  
> 当数据量少，特征非常多的时候，容易出现**过拟合**，这时要通过**正则化**调整。

        [正则化线性回归]([(7条消息) 正则化(线性回归)_小星爷的博客-CSDN博客](https://blog.csdn.net/qq_23371241/article/details/78373888))

23. 关联分析

24. 混合高斯聚类
    
    > $\color{red}{EM算法}$：期望最大化算法，被用来学习高斯混合模型（Gaussian mixture model，简称GMM）的参数；隐式马尔科夫算法（HMM）、LDA主题模型的变分推断等等。
    > 
    > EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为**期望步（E步）**，另一个为**极大步（M步）**，所以算法被称为EM算法。EM算法受到缺失思想影响，最初是为了**解决数据缺失情况**下的参数估计问题，其算法基础和收敛有效性等问题在Dempster、Laird和Rubin三人于1977年所做的文章《Maximum likelihood from incomplete data via the EM algorithm》中给出了详细的阐述。其基本思想是：**首先根据己经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束**。
    
    [混合高斯聚类理解]([(7条消息) 【机器学习笔记】通俗易懂解释高斯混合聚类原理_lotusng的博客-CSDN博客](https://blog.csdn.net/lotusng/article/details/79990724))

       [EM算法]([EM算法详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/40991784))

25. 主成分分析是一种降维方法

26. PCA做降维处理时，优先处理 中心化样本的协方差矩阵的最大特征值对应特征向量
    
    > $\color{red}{PCA}$(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是**从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的**。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。
    
    > [PCA]([主成分分析（PCA）原理详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37777074))

27. 过拟合：训练样本测试误差小，测试样本正确识别率很低

28. 马尔科夫链

29. pass

30. pass

31. pass

32. 梯度下降算法正确步骤
    
    > 原理：**寻找损失函数的最低点**，就像我们在山谷里行走，希望找到山谷里最低的地方。那么如何寻找损失函数的最低点呢？在这里，我们使用了微积分里导数，通过求出函数导数的值，从而找到函数下降的方向或者是最低点（极值点）
    
    > 步骤：
    > 
    > 1. 初始化随即权重和偏差
    > 2. 将输入传入网络，得到输出值
    > 3. 计算预测值于真实值之间的误差
    > 4. 对产生误差的神经元，改变相应权重值以减小误差
    > 5. 迭代更新，直到找到最佳权重
    > 
    >     $w_{i+1}=w_{i}-\alpha*\frac{L}{dw_i}$
    > 
    >     $b_{i+1}=b_i-\alpha*\frac{dL}{db_i}$

       [markdown数学公式总结]([(8条消息) markdown最全数学公式速查_xkgjfl的博客-CSDN博客](https://blog.csdn.net/jyfu2_12/article/details/79207643#:~:text=Markdown%20%E6%95%B0%E5%AD%A6%E8%AF%AD%E6%B3%95%201.%E5%9F%BA%E6%9C%AC%E6%A0%BC%E5%BC%8F%201.%E8%A1%8C%E5%86%85%E5%85%AC%E5%BC%8F%20%E4%BB%A5%20%24%20%E5%BC%80%E5%A4%B4%EF%BC%8C%E4%BB%A5,%24%20%E7%BB%93%E5%B0%BE%E3%80%82%20eg%3A%20aba%5Ebab%20%24a%5Eb%24%202.%E5%9D%97%E7%BA%A7%E5%85%AC%E5%BC%8F%E2%80%93%E8%A1%8C%E9%97%B4%E5%85%AC%E5%BC%8F%20%E4%BB%A5))

       [markdown常见公式]([(8条消息) 一、markdown 常见公式_林重言的博客-CSDN博客](https://blog.csdn.net/linshen1213/article/details/115330264?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-115330264-blog-79207643.pc_relevant_recovery_v2&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-115330264-blog-79207643.pc_relevant_recovery_v2&utm_relevant_index=1))

       [梯度下降算法]([梯度下降算法 - 搜索 (bing.com)](https://cn.bing.com/search?q=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95&qs=n&form=QBRE&sp=-1&pq=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95&sc=10-6&sk=&cvid=08A6A8953DF24DDAA898BBE01BC20C51&ghsh=0&ghacc=0&ghpl=))

33. 岭回归
    
    > $\color {red} {方差}$： 都是围着数据中心的，方差越大则表示距离数据中心分布的越分散，越小说明越近越集中
    > 
    > $\color{red}{偏差}$： 偏离数据中心， 偏差越大，说明整个数据距离中心越远，偏差越小，说明距离数据中心越近。
    > 
    > 这两者的关系通常是**矛盾**的，降低偏差会提高方差，降低方差会提高偏差。所有一个好的模型就是对这点的一个平衡。 
    
    > $\color{red}{岭回归}$ :针对高方差，即过拟合的模型，解决办法之一就是对模型进行正则化：限制参数大小。当线性回归过拟合时，权重系数wj就会非常的大，岭回归就是要解决这样的问题。岭回归可以理解为在线性回归的损失函数的基础上，加,入一个L2正则项，来限制W不要过大。其中λ>0，通过确定λ的值可以使得模型在偏差和方差之间达到平衡，**随着λ的增大，模型的方差减小，偏差增大**。

<img title="" src="file:///C:/Users/田晓滨/AppData/Roaming/marktext/images/2022-11-19-12-00-31-image.png" alt="" data-align="center" width="324">

        [岭回归]([学习笔记233—岭回归和Lasso回归区别 - 何弈 - 博客园 (cnblogs.com)](https://www.cnblogs.com/hechangchun/p/15194557.html#:~:text=%E5%B2%AD%E5%9B%9E%E5%BD%92%EF%BC%88Ridge,Regression%EF%BC%89%E5%8F%AF%E4%BB%A5%E7%90%86%E8%A7%A3%E4%B8%BA%E5%9C%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%EF%BC%8C%E5%8A%A0%2C%E5%85%A5%E4%B8%80%E4%B8%AAL2%E6%AD%A3%E5%88%99%E9%A1%B9%EF%BC%8C%E6%9D%A5%E9%99%90%E5%88%B6W%E4%B8%8D%E8%A6%81%E8%BF%87%E5%A4%A7%E3%80%82%20%E5%85%B6%E4%B8%AD%CE%BB%3E0%EF%BC%8C%E9%80%9A%E8%BF%87%E7%A1%AE%E5%AE%9A%CE%BB%E7%9A%84%E5%80%BC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E5%BE%97%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%E4%B9%8B%E9%97%B4%E8%BE%BE%E5%88%B0%E5%B9%B3%E8%A1%A1%EF%BC%8C%E9%9A%8F%E7%9D%80%CE%BB%E7%9A%84%E5%A2%9E%E5%A4%A7%EF%BC%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%96%B9%E5%B7%AE%E5%87%8F%E5%B0%8F%EF%BC%8C%E5%81%8F%E5%B7%AE%E5%A2%9E%E5%A4%A7%E3%80%82))

34. 增加模型的欠拟合风险
    
    > $\color{red}{过拟合}$
    > 
    > 原因：
    > 
    > 训练数据中**噪音干扰过大**，使得学习器认为部分噪音是特征从而扰乱学习规则。
    > 
    > 建模**样本选取有误**，例如训练数据太少，抽样方法错误，样本label错误等，导致样本不能代表整体。
    > 
    > **模型不合理**，或假设成立的条件与实际不符。
    > 
    > **特征维度/参数太多**，导致模型复杂度太高。  
    > 
    >  解决方法：
    > 
    > **从数据源头获取更多数据**
    > 
    > **根据当前数据集估计数据分布参数，使用该分布产生更多数据**。这个方法一般不用，因为估计参数分布的过程也会带入抽样误差。
    > 
    > **数据增强**（Data Augmentation）:通过一定规则扩充数据。如物体在图像中的位置、姿态、尺度、整体图片明暗度等都不会影响分类结果。我们可以通过图像平移、反转、缩放、切割等手段将数据库成倍扩充
    > 
    > **保留验证集**
    > 
    > **获取额外数据进行交叉验证**
    > 
    > **降低模型复杂度**
    > a. 对于神经网络：减少网络的层数、神经元个数等均可以限制网络的拟合能力。dropout，在向前传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型的泛化性更强，因为它不会太依赖某些局部的特征。。
    > b. 对于决策树：限制树深，剪枝，限制叶节点数量。
    > c. 增大分割平面间隔
    > 
    > **特征选择、特征降维**
    > 
    > **early stopping**
    > 
    > **正则化**（限制权值weight-decay）：将权值的大小作为惩罚项加入到损失函数里。
    > 
    > **增加噪声**
    > **ensemble**
    > 
    > $\color{red}{欠拟合}$
    > 
    > 原因：
    > 
    > **模型复杂度过低**
    > **特征量过少**
    > 
    > 解决方法：
    > 
    > **增加特征数**；
    > 当特征不足或者现有特征与样本标签的相关性不强时，模型易出现欠拟合。
    > 可以通过挖掘上下文特征，ID类特征，组合特征等新的特征，可以取得较好的效果。这属于特征工程相关的内容，如因子分解机，梯度提升决策树，deep_crossing都可以丰富特征。
    > 
    > **增加模型复杂度**；
    > 模型简单时其表达能力较差，容易导致欠拟合，因此可以适当地增加模型复杂度，使模型拥有更强的拟合能力。
    > 如线性模型中添加高次项，神经网络中增加网络层数或神经元个数。尝试非线性模型，比如核SVM 、决策树、DNN等模型。
    > 
    > **减小正则化系数**。
    > 正则化是用于防止过拟合的，但是当出现欠拟合时，就有必要针对性地减小正则化系数。

        [欠拟合和过拟合原因+解决方法]([(8条消息) 过拟合与欠拟合_Minouio的博客-CSDN博客](https://blog.csdn.net/qq_42012732/article/details/107318550))

35. Bagging每轮训练的样本权重相同，而Bootsting则根据单个分类器的正确率选择权重
    
    EM算法和梯度下降都会陷于局部极小值
    
    梯度下降也可用于求混合高斯模型的参数(一般不适用)     

36. 随机森林模型过拟合与决策树**深度、叶节点数**有关

37. pass

38. pass

39. K-means算法
    
    > K均值聚类是最基础常用的聚类算法，它的基本思想是，**通过迭代寻找K个簇的一种划分方案，使得聚类结果对应的损失函数最小**。
    > 
    > KMenas的优点：
    > 
    > - 高效可伸缩，计算复杂度 为接近于线性（N是数据量，K是聚类总数，t是迭代轮数）。
    > - 收敛速度快，原理相对通俗易懂，可解释性强。
    > 
    > KMeans也有一些明显的缺点：
    > 
    > - 受初始值和异常点影响，聚类结果可能不是全局最优而是局部最优。
    > - K是超参数，一般需要按经验选择
    > - 样本点只能划分到单一的类中 
    > 
    > 迭代的时间复杂度$\Omicron(n)$ 
    > 
    > K-means可以使用核函数，就是将数据点都投影到了一个高维的特征空间中（为啥要这么做呢，主要是突显出不同样本中的差异），然后再在这个高维的特征空间中，进行传统的k-means聚类
    > 
    > K-mean适用于当簇是密集的、球状或团状的，而簇与簇之间区别明显时，使用[k-mean](https://www.zhihu.com/search?q=k-mean&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A980863878%7D)聚类效果很好；在应用[欧式距离](https://www.zhihu.com/search?q=%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A980863878%7D)时，只能找到数据点分布的比较均匀的簇。

        [K-means]([KMeans聚类算法详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/184686598))

       [核K-means]([weighted Kernel k-means 加权核k均值算法理解及其实现（一） - 苏白 - 博客园 (cnblogs.com)](https://www.cnblogs.com/subaiBlog/p/6271315.html#:~:text=%E6%A0%B8k-means%EF%BC%8C%E6%A6%82%E6%8B%AC%E5%9C%B0%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%B0%86%E6%95%B0%E6%8D%AE%E7%82%B9%E9%83%BD%E6%8A%95%E5%BD%B1%E5%88%B0%E4%BA%86%E4%B8%80%E4%B8%AA%E9%AB%98%E7%BB%B4%E7%9A%84%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E4%B8%AD%EF%BC%88%E4%B8%BA%E5%95%A5%E8%A6%81%E8%BF%99%E4%B9%88%E5%81%9A%E5%91%A2%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E7%AA%81%E6%98%BE%E5%87%BA%E4%B8%8D%E5%90%8C%E6%A0%B7%E6%9C%AC%E4%B8%AD%E7%9A%84%E5%B7%AE%E5%BC%82%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E5%9C%A8%E8%BF%99%E4%B8%AA%E9%AB%98%E7%BB%B4%E7%9A%84%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E4%B8%AD%EF%BC%8C%E8%BF%9B%E8%A1%8C%E4%BC%A0%E7%BB%9F%E7%9A%84k-means%E8%81%9A%E7%B1%BB%E3%80%82%20%E4%B8%BB%E8%A6%81%E7%9A%84%E6%80%9D%E6%83%B3%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B9%88%E7%AE%80%E5%8D%95%EF%BC%8C%E6%AF%94%E8%B5%B7%E4%BC%A0%E7%BB%9F%E6%99%AE%E9%80%9A%E7%9A%84k-means%E5%B0%B1%E5%A4%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E6%AD%A5%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E6%93%8D%E4%BD%9C%E3%80%82%20%E6%89%80%E4%BB%A5%E5%AE%83%E7%9A%84%E5%85%AC%E5%BC%8F%E4%B9%9F%E4%B8%8E%E4%BC%A0%E7%BB%9Fk-means%E5%BE%88%E7%9B%B8%E8%BF%91%EF%BC%9A%20%E8%BF%99%E9%87%8C%E6%89%80%E8%A1%A8%E7%A4%BA%E7%9A%84%EF%BC%8C%24a_i%24%E6%98%AF%E5%90%84%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%82%B9%EF%BC%8C%E8%80%8C%24phi,%28%20%29%24%E5%87%BD%E6%95%B0%EF%BC%8C%E5%88%99%E6%98%AF%E8%A1%A8%E7%A4%BA%E5%B0%86%E6%A0%B7%E6%9C%AC%E7%82%B9%E6%98%A0%E5%B0%84%E8%87%B3%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4%E3%80%82%20%E8%80%8C%E5%85%AC%E5%BC%8F%E4%B8%AD%E7%9A%84%24m_c%24%EF%BC%8C%E5%88%99%E6%98%AF%E6%AF%8F%E4%B8%AA%E7%B0%87%E4%B8%AD%E7%9A%84%E8%B4%A8%E5%BF%83%E3%80%82%20%E8%BF%99%E4%B8%AA%E5%BD%A2%E5%BC%8F%EF%BC%8C%E5%AF%B9%E6%AF%94%E8%B5%B7%E6%99%AE%E9%80%9A%E7%9A%84k-means%E6%98%AF%E4%B8%8D%E6%98%AF%E5%BE%88%E7%9B%B8%E8%BF%91%E5%92%A7%EF%BC%8C%E5%90%8C%E6%A0%B7%E6%98%AF%E6%B1%82%E5%90%84%E4%B8%AA%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%A0%B7%E6%9C%AC%E7%89%B9%E5%BE%81%EF%BC%88%E4%BC%A0%E7%BB%9Fk-means%E4%B8%AD%E6%98%AF%E6%A0%B7%E6%9C%AC%E7%82%B9%EF%BC%89%E4%B8%8E%E8%B4%A8%E5%BF%83%E7%9A%84%E8%B7%9D%E7%A6%BB%EF%BC%8C%E7%84%B6%E5%90%8E%E6%A0%B9%E6%8D%AE%E8%AF%A5%E6%A0%B7%E6%9C%AC%E7%82%B9%E5%88%B0%E4%B8%8D%E5%90%8C%E8%B4%A8%E5%BF%83%E8%B7%9D%E7%A6%BB%EF%BC%8C%E9%80%89%E6%8B%A9%E6%9C%80%E5%B0%8F%E7%9A%84%EF%BC%88%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E8%B7%9D%E7%A6%BB%E8%AF%A5%E6%A0%B7%E6%9C%AC%E7%82%B9%E6%9C%80%E8%BF%91%EF%BC%89%E7%9A%84%E7%B0%87%E5%BF%83%EF%BC%8C%E8%BF%9B%E8%A1%8C%E5%88%86%E9%85%8D%EF%BC%88%E5%B0%86%E8%AF%A5%E6%A0%B7%E6%9C%AC%E7%82%B9%E5%BD%92%E4%B8%BA%E8%BF%99%E4%B8%AA%E7%B0%87%EF%BC%89%E3%80%82))

40. 过拟合训练误差小，测试误差大

41. pass

42. 过拟合问题 见P34
    
    > **稀疏**表示的好处在于降低表示复杂度，更直白**的**原因其实就是**减少**系数参数，通过**稀疏**表示，可以充分发挥数据所含有**的**信息，去掉冗余**的**数据信息，达到最大化利用数据。
    > 
    > 高斯核比线性核更复杂，使复杂度更高，容易导致过拟合问题。

        [高斯核和线性核]([核函数 高斯核函数，线性核函数，多项式核函数_TranSad的博客-CSDN博客](https://blog.csdn.net/weixin_44492824/article/details/122546701))

43. 无监督和监督

44. Bootstrap：Bootstrap法具体是指**用原样本自身的数据**抽样得出新的样本及统计量

45. Adaboost算法
    
    > $\color{red}{原理}$:是从训练数据中学习出一系列的弱分类器，并将弱分类器组合成一个强分类器
    > 
    > $\color{red}{提升树}$：采用**加法**模型（基函数的线性组合）与**前向分布**算法，同时以决策树为基函数的提升方法。对于分类问题而言是二叉分类树，但对于回归问题而言是二叉回归树

46. HMM

47. 会考虑向量的方向的距离
    
    > Jaccard距离：杰卡德距离
    
    [四种距离]([欧氏距离，曼哈顿距离，余弦距离，汉明距离_南极洲闲人的博客-CSDN博客_汉明距离和欧式距离](https://blog.csdn.net/weixin_45965358/article/details/112992351))
    
    [机器学习中的距离]([机器学习中的数学——距离定义（十三）：杰卡德距离（Jaccard Distance）和杰卡德相似系数（Jaccard Similarity Coefficient）_von Neumann的博客-CSDN博客](https://blog.csdn.net/hy592070616/article/details/122279515))
